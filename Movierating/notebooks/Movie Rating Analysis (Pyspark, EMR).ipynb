{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Rating Analytics:\n",
    "\n",
    "In this notebook, I will perform data analysis on movie data. We use spark on python to develop this application. Instruction [here](https://www.udemy.com/taming-big-data-with-apache-spark-hands-on/learn/v4/t/lecture/3700612?start=0) was used to setup pyspark and perform analysis.\n",
    "\n",
    "Data is downloaded from [Grouplens](https://grouplens.org/).\n",
    "\n",
    "#### Problem 1:\n",
    "Develope a histogram of movie ratings. What is the frequenct of each rating?\n",
    "\n",
    "``I am saving codes into a new file and run it through jupyter notebook inline command line. This lets us not define sparc context for every tas``.\n",
    "Each line of the data set has the following columns:\n",
    "\n",
    "- ``User_id``\n",
    "- ``Movie_id``\n",
    "- ``Rating``\n",
    "- ``Timestamp``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting codes/rating_hist.py\n"
     ]
    }
   ],
   "source": [
    "%%file codes/rating_hist.py\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkConf, SparkContext  # sparkcontext is the entry points to RDD\n",
    "# spark conf allows congifuring spark context\n",
    "from collections import OrderedDict # to save the order that entries are added\n",
    "import matplotlib.pyplot as plt\n",
    "conf = SparkConf().setMaster('local').setAppName('Histogram') # configuring on one single core on machine\n",
    "sc = SparkContext(conf = conf)  # Main entry point for Spark\n",
    "\n",
    "# importing the data into RDD.\n",
    "# no need to add C:/\n",
    "lines = sc.textFile('file:///Users/Amin/Dropbox/Career Deveoment/Data Science/PySpark/Movierating/data/raw/ml-100k/u.data')\n",
    "# mapping line into rating values=\n",
    "# map performs one to one between input and output\n",
    "ratings = lines.map(lambda x: x.strip().split('\\t')[2])\n",
    "# reducing based on the provided value (no key is available)\n",
    "histogram = ratings.countByValue()\n",
    "ordered_hist = OrderedDict(sorted(histogram.items(), key = lambda t:t[1], reverse=True)) # sort them by largest values\n",
    "\n",
    "ratings = []\n",
    "frequencies = []\n",
    "for rating, frequency in ordered_hist.items():\n",
    "    print('Rating *{}* has {} entry.'.format(rating, frequency))\n",
    "    ratings.append(str(rating))\n",
    "    frequencies.append(frequency)\n",
    "    \n",
    "    \n",
    "plt.bar(ratings, frequencies)\n",
    "plt.xlabel('Ratings')\n",
    "plt.ylabel('Frequencies')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rating *4* has 34174 entry.\n",
      "Rating *3* has 27144 entry.\n",
      "Rating *5* has 21201 entry.\n",
      "Rating *2* has 11370 entry.\n",
      "Rating *1* has 6110 entry.\n",
      "Rating *23* has 1 entry.\n",
      "Figure(640x480)\n",
      "SUCCESS: The process with PID 19532 (child process of PID 988) has been terminated.\n",
      "SUCCESS: The process with PID 988 (child process of PID 16332) has been terminated.\n",
      "SUCCESS: The process with PID 16332 (child process of PID 8664) has been terminated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19/03/10 11:02:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\n",
      "                                                                                \n"
     ]
    }
   ],
   "source": [
    "! python codes/rating_hist.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 2: \n",
    "Find the movie that has the highest average and is watched more than 100 times.\n",
    "We are also going to use the concept of broadcasting to pring out movie name instead of movie id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting codes/top_movies.py\n"
     ]
    }
   ],
   "source": [
    "%%file codes/top_movies.py\n",
    "\n",
    "# we need to perform table join between movie names and movie ids\n",
    "# developing a look up table for movie names and movie ids \n",
    "import os\n",
    "def movie_lut(): # movie look-up-table\n",
    "    file = os.path.join(os.getcwd(), r'..', r'data\\raw\\ml-100k\\u.item')\n",
    "    with open(file) as items:\n",
    "        movie_dict = dict()  # initializing the look up table\n",
    "        for line in items:\n",
    "            data = line.strip().split('|')  # the file is pipe delimiated\n",
    "            movie_id = data[0]\n",
    "            movie_name = data[1]\n",
    "            movie_dict[movie_id] = movie_name\n",
    "    return(movie_dict)\n",
    "\n",
    "look_up_table = movie_lut()\n",
    "\n",
    "# accessing pyspark module\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# configure and connect to spark context\n",
    "from pyspark import SparkConf, SparkContext\n",
    "conf = SparkConf().setMaster('local[4]').setAppName('TopMovies')\n",
    "sc = SparkContext(conf = conf)\n",
    "# broadcast the look up table to all the nodes\n",
    "boradcasted_lut = sc.broadcast(look_up_table)\n",
    "# improt the data into RDD\n",
    "movies = sc.textFile('file:///Users/Amin/Dropbox/Career Deveoment/Data Science/PySpark/Movierating/data/raw/ml-100k/u.data')\n",
    "# parse the lines\n",
    "movie_rating = movies.map(lambda x: x.strip().split('\\t')[1:3]).map(lambda x: (x[0], (float(x[1]),1))) # appended 1 to simplify averaging\n",
    "# perform required mapping and reducing\n",
    "movie_by_rating = movie_rating.reduceByKey(lambda x, y: (x[0]+y[0], x[1]+y[1])).filter(lambda x: x[1][1] > 100).mapValues(lambda x: round(x[0] / x[1],3)) # average\n",
    "sorted_movies = movie_by_rating.sortBy(lambda x: x[1], ascending = False).map(lambda x: (boradcasted_lut.value[x[0]], x[1])) # replacing movie id by movie name\n",
    "# collect the results\n",
    "#top_movie = sorted_movies.first() # first sorted element\n",
    "# output the results\n",
    "results = sorted_movies.take(10)\n",
    "print(\"Movie:\" + \"\\t\" + 'Avg Rating')\n",
    "print('-'*30)\n",
    "for movie, avgrat in results:\n",
    "    print(movie, \":\\t\", avgrat)\n",
    "    \n",
    "# closet the context \n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie:\tAvg Rating\n",
      "------------------------------\n",
      "Close Shave, A (1995) :\t 4.491\n",
      "Schindler's List (1993) :\t 4.466\n",
      "Wrong Trousers, The (1993) :\t 4.466\n",
      "Casablanca (1942) :\t 4.457\n",
      "Shawshank Redemption, The (1994) :\t 4.445\n",
      "Rear Window (1954) :\t 4.388\n",
      "Usual Suspects, The (1995) :\t 4.386\n",
      "Star Wars (1977) :\t 4.358\n",
      "12 Angry Men (1957) :\t 4.344\n",
      "Citizen Kane (1941) :\t 4.293\n",
      "SUCCESS: The process with PID 9656 (child process of PID 4232) has been terminated.\n",
      "SUCCESS: The process with PID 4232 (child process of PID 6404) has been terminated.\n",
      "SUCCESS: The process with PID 6404 (child process of PID 8016) has been terminated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19/03/16 13:43:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "\n",
      "[Stage 0:>                                                          (0 + 2) / 2]\n",
      "[Stage 0:=============================>                             (1 + 1) / 2]\n",
      "[Stage 1:>                                                          (0 + 2) / 2]\n",
      "                                                                                \n",
      "\n",
      "[Stage 3:>                                                          (0 + 2) / 2]\n",
      "                                                                                \n",
      "\n",
      "[Stage 5:>                                                          (0 + 2) / 2]\n",
      "[Stage 6:>                                                          (0 + 1) / 1]\n",
      "                                                                                \n"
     ]
    }
   ],
   "source": [
    "!python codes/top_movies.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probelm 3\n",
    "We are doing the same logic, but on AWS EMR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting codes/top_movies_1m.py\n"
     ]
    }
   ],
   "source": [
    "%%file codes/top_movies_1m.py\n",
    "\n",
    "# to run on EMR successfuly first copy the files to the current working directoy (master node)\n",
    "# aws s3 cp s3://pyspark-0000/top_movies_1m.py ./\n",
    "# aws s3 cp s3://pyspark-0000/movies.dat ./\n",
    "#spark-submit --executor-memory 1g top_movies_1m.py\n",
    "# we need to perform table join between movie names and movie ids\n",
    "# developing a look up table for movie names and movie ids \n",
    "def movie_lut(): # movie look-up-table\n",
    "    file = 'movies.dat'\n",
    "    with open(file) as items:\n",
    "        movie_dict = dict()  # initializing the look up table\n",
    "        for line in items:\n",
    "            data = line.strip().split('::')  # the file is pipe delimiated\n",
    "            movie_id = data[0]\n",
    "            movie_name = data[1]\n",
    "            movie_dict[movie_id] = movie_name\n",
    "    return(movie_dict)\n",
    "\n",
    "look_up_table = movie_lut()\n",
    "\n",
    "# accessing pyspark module\n",
    "#import findspark\n",
    "#findspark.init()\n",
    "\n",
    "# configure and connect to spark context\n",
    "from pyspark import SparkConf, SparkContext\n",
    "conf = SparkConf()# not configuring sparcontext automatically tell spark to go on pyspark yarn\n",
    "sc = SparkContext(conf = conf)\n",
    "# broadcast the look up table to all the nodes\n",
    "boradcasted_lut = sc.broadcast(look_up_table)\n",
    "# improt the data into RDD\n",
    "movies = sc.textFile('s3://pyspark-0000/ratings.dat')\n",
    "# parse the lines\n",
    "movie_rating = movies.map(lambda x: x.strip().split('::')[1:3]).map(lambda x: (x[0], (float(x[1]),1))) # appended 1 to simplify averaging\n",
    "# perform required mapping and reducing\n",
    "movie_by_rating = movie_rating.reduceByKey(lambda x, y: (x[0]+y[0], x[1]+y[1])).filter(lambda x: x[1][1] > 100).mapValues(lambda x: round(x[0] / x[1],3)) # average\n",
    "sorted_movies = movie_by_rating.sortBy(lambda x: x[1], ascending = False).map(lambda x: (boradcasted_lut.value[x[0]], x[1])) # replacing movie id by movie name\n",
    "# collect the results\n",
    "#top_movie = sorted_movies.first() # first sorted element\n",
    "# output the results\n",
    "results = sorted_movies.take(10)\n",
    "print(\"Movie:\" + \"\\t\" + 'Avg Rating')\n",
    "print('-'*30)\n",
    "for movie, avgrat in results:\n",
    "    print(movie, \":\\t\", avgrat)\n",
    "    \n",
    "# closet the context \n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
