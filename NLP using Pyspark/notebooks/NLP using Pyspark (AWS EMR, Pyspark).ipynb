{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP using Pyspark\n",
    "\n",
    "The goal of this notebook is to peform basic NLP tasks using Pyspark. We are perforing these tasks on a few books. Though these books are small in size and are not considered big data, the same procedure could be applied to much larger data sets such as forum data, log data, etc.\n",
    "\n",
    "``flatMap`` mapper is very important in the text processing problems as it allows mapping each line into many elements (one-to-many mapper).\n",
    "\n",
    "#### Probelm 1:\n",
    "List top 20 words used in the Pride and Prejudice book, sorted alphabetically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting codes/word_count.py\n"
     ]
    }
   ],
   "source": [
    "%%file codes/word_count.py\n",
    "\n",
    "\n",
    "# finding pyspark\n",
    "import findspark\n",
    "findspark.init() # adding pyspark to sys.path\n",
    "# importing require modules\n",
    "from pyspark import SparkConf, SparkContext\n",
    "# importing regex to perform tokenization\n",
    "import re\n",
    "# setting up spark environment\n",
    "conf = SparkConf().setMaster('local[4]').setAppName('WordCounter')\n",
    "sc = SparkContext(conf = conf)\n",
    "# importing the file\n",
    "lines = sc.textFile('file:///Users/Amin/Dropbox/Career Deveoment/Data Science/PySpark/NLP using Pyspark/data/raw/pride_prejudice.txt')\n",
    "# defining the text parser\n",
    "def parser(line):\n",
    "    '''\n",
    "    Performs the parsing of each line of the book.\n",
    "    Input is a line of text and output is a list of words in that line.'''\n",
    "    line = line.strip().lower() # the application is not case sensitive\n",
    "    punctuations = re.compile(r'[^a-z]+') # tokenize on non-alphabetical  (\\W+ can also be used)\n",
    "    tokens = punctuations.split(line)\n",
    "    tokens = [word for word in tokens if word]\n",
    "    return tokens\n",
    "\n",
    "def full_line(line):\n",
    "    '''\n",
    "    checks if the line has text.'''\n",
    "    if line:\n",
    "        return True\n",
    "    else: False\n",
    "# perform mapping\n",
    "filtered_lines = lines.filter(full_line)\n",
    "tokens_one = filtered_lines\\\n",
    ".flatMap(parser).map(lambda x: (x,1)) # tokenization and counting\n",
    "# perform reduction\n",
    "tokens_count = tokens_one.reduceByKey(lambda x, y : x+ y)\n",
    "# collect the results\n",
    "results = tokens_count.collect()\n",
    "# output 20 most frequent words\n",
    "sorted_alpha = sorted(results, key = lambda x: x[0]) # sorting alphabetically\n",
    "sorted_results = sorted(sorted_alpha, reverse = True, key = lambda x: x[1]) # sorting according to frequncy\n",
    "top20 = sorted_results[:20]\n",
    "for i, wc in enumerate(top20):\n",
    "    print('Word-{}: {} \\t {} times.'.format(i+1, wc[0], wc[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word-1: the \t 4507 times.\n",
      "Word-2: to \t 4243 times.\n",
      "Word-3: of \t 3730 times.\n",
      "Word-4: and \t 3658 times.\n",
      "Word-5: her \t 2225 times.\n",
      "Word-6: i \t 2070 times.\n",
      "Word-7: a \t 2011 times.\n",
      "Word-8: in \t 1937 times.\n",
      "Word-9: was \t 1847 times.\n",
      "Word-10: she \t 1710 times.\n",
      "Word-11: that \t 1594 times.\n",
      "Word-12: it \t 1550 times.\n",
      "Word-13: not \t 1450 times.\n",
      "Word-14: you \t 1428 times.\n",
      "Word-15: he \t 1339 times.\n",
      "Word-16: his \t 1271 times.\n",
      "Word-17: be \t 1260 times.\n",
      "Word-18: as \t 1192 times.\n",
      "Word-19: had \t 1177 times.\n",
      "Word-20: with \t 1100 times.\n",
      "SUCCESS: The process with PID 13500 (child process of PID 10272) has been terminated.\n",
      "SUCCESS: The process with PID 10272 (child process of PID 13528) has been terminated.\n",
      "SUCCESS: The process with PID 13528 (child process of PID 5276) has been terminated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19/03/10 23:39:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "\n",
      "[Stage 0:>                                                          (0 + 2) / 2]\n",
      "[Stage 0:=============================>                             (1 + 1) / 2]\n",
      "[Stage 1:>                                                          (0 + 2) / 2]\n",
      "[Stage 1:=============================>                             (1 + 1) / 2]\n",
      "                                                                                \n"
     ]
    }
   ],
   "source": [
    "!python codes/word_count.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's perform the same logic while performing the sort in pyspark so that we can keep our code scalable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing codes/scalable_wc.py\n"
     ]
    }
   ],
   "source": [
    "%%file codes/scalable_wc.py\n",
    "\n",
    "\n",
    "# finding pyspark\n",
    "import findspark\n",
    "findspark.init() # adding pyspark to sys.path\n",
    "# importing require modules\n",
    "from pyspark import SparkConf, SparkContext\n",
    "# importing regex to perform tokenization\n",
    "import re\n",
    "# setting up spark environment\n",
    "conf = SparkConf().setMaster('local[4]').setAppName('WordCounter')\n",
    "sc = SparkContext(conf = conf)\n",
    "# importing the file\n",
    "lines = sc.textFile('file:///Users/Amin/Dropbox/Career Deveoment/Data Science/PySpark/NLP using Pyspark/data/raw/pride_prejudice.txt')\n",
    "# defining the text parser\n",
    "def parser(line):\n",
    "    '''\n",
    "    Performs the parsing of each line of the book.\n",
    "    Input is a line of text and output is a list of words in that line.'''\n",
    "    line = line.strip().lower() # the application is not case sensitive\n",
    "    punctuations = re.compile(r'[^a-z]+') # tokenize on non-alphabetical  (\\W+ can also be used)\n",
    "    tokens = punctuations.split(line)\n",
    "    tokens = [word for word in tokens if word]\n",
    "    return tokens\n",
    "\n",
    "def full_line(line):\n",
    "    '''\n",
    "    checks if the line has text.'''\n",
    "    if line:\n",
    "        return True\n",
    "    else: False\n",
    "# perform mapping\n",
    "filtered_lines = lines.filter(full_line)\n",
    "tokens_one = filtered_lines\\\n",
    ".flatMap(parser).map(lambda x: (x,1)) # tokenization and counting\n",
    "# perform reduction\n",
    "tokens_count = tokens_one.reduceByKey(lambda x, y : x+ y)\n",
    "# collect the results\n",
    "sorted_flipped = tokens_count.map(lambda x, y : (y,x)).sortByKey(ascending = False)\n",
    "sorted_correct = sorted_flipped.map(lambda x, y : (y, x))\n",
    "sorted_results = sorted_correct.collect()\n",
    "\n",
    "\n",
    "for res in sorted_results:\n",
    "    count = res[1]\n",
    "    word = res[0]\n",
    "    print(word + \":\\t\\t\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python codes/scalable_wc.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
